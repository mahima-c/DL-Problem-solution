{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Many-to-One â€“ Sentiment Analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPVsP+9L43sP+daZPvBh4KL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahima-c/DL-Problem-solution/blob/main/Many_to_One_%E2%80%93_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s55SkI_ZvyPU"
      },
      "source": [
        "Our dataset is the Sentiment labeled sentences dataset on the UCI Machine Learning Repository [20], a set of 3,000 sentences from reviews on Amazon, IMDb, and Yelp, each labeled with 0 if it expresses a negative sentiment, or 1 if it expresses a positive sentiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keplzHb5vqWM"
      },
      "source": [
        "import numpy as np\r\n",
        "import os\r\n",
        "import shutil\r\n",
        "import tensorflow as tf\r\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxCp8xdxFCLz"
      },
      "source": [
        "def clean_logs(data_dir):\r\n",
        "    logs_dir = os.path.join(data_dir, \"logs\")\r\n",
        "    shutil.rmtree(logs_dir, ignore_errors=True)\r\n",
        "    return logs_dir\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAQda_QQwYy5"
      },
      "source": [
        "The dataset is provided as a zip file, which expands into a folder containing three files of labeled sentences, one for each provider, with one sentence and label per line, with the sentence and label separated by the tab character. We first download the zip file, then parse the files into a list of (sentence, label) pairs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8w8IAYwwZIM",
        "outputId": "983701ba-76ce-495f-8b06-b90aa020fd23"
      },
      "source": [
        "def download_and_read(url):\r\n",
        "   local_file = url.split('/')[-1]\r\n",
        "   local_file = local_file.replace(\"%20\", \" \")\r\n",
        "   p = tf.keras.utils.get_file(local_file, url,\r\n",
        "       extract=True, cache_dir=\".\")\r\n",
        "   local_folder = os.path.join(\"datasets\", local_file.split('.')[0])\r\n",
        "   labeled_sentences = []\r\n",
        "   for labeled_filename in os.listdir(local_folder):\r\n",
        "       if labeled_filename.endswith(\"_labelled.txt\"):\r\n",
        "           with open(os.path.join(\r\n",
        "                   local_folder, labeled_filename), \"r\") as f:\r\n",
        "               for line in f:\r\n",
        "                   sentence, label = line.strip().split('\\t')\r\n",
        "                   labeled_sentences.append((sentence, label))\r\n",
        "   return labeled_sentences\r\n",
        "labeled_sentences = download_and_read(      \r\n",
        "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/\" + \r\n",
        "    \"00331/sentiment%20labelled%20sentences.zip\")\r\n",
        "sentences = [s for (s, l) in labeled_sentences]\r\n",
        "labels = [int(l) for (s, l) in labeled_sentences]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\n",
            "90112/84188 [================================] - 0s 3us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GzdF3XnyHHV"
      },
      "source": [
        "Our objective is to train the model so that, given a sentence as input, it learns to predict the corresponding sentiment provided in the label. Each sentence is a sequence of words. However, in order to input it into the model, we have to convert it into a sequence of integers. Each integer in the sequence will point to a word. The mapping of integers to words for our corpus is called a vocabulary. Thus we need to tokenize the sentences and produce a vocabulary. This is done using the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb1VS2HPyHor",
        "outputId": "d9d49729-805d-4a8c-fa4e-1e68d3650641"
      },
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\r\n",
        "tokenizer.fit_on_texts(sentences)\r\n",
        "vocab_size = len(tokenizer.word_counts)\r\n",
        "print(\"vocabulary size: {:d}\".format(vocab_size))\r\n",
        "word2idx = tokenizer.word_index\r\n",
        "idx2word = {v:k for (k, v) in word2idx.items()}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocabulary size: 5271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZrSvkWqyrBH"
      },
      "source": [
        "Our vocabulary consists of 5271 unique words. It is possible to make the size smaller by dropping words that occur fewer than some threshold number of times, which can be found by inspecting the tokenizer.word_counts dictionary. In such cases, we need to add 1 to the vocabulary size for the UNK (unknown) entry, which will be used to replace every word that is not found in the vocabulary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyDvtsWuzMmu"
      },
      "source": [
        "Each sentence can have a different number of words. Our model will require us to provide sequences of integers of identical length for each sentence. In order to support this requirement, it is common to choose a maximum sequence length that is large enough to accommodate most of the sentences in the training set. Any sentences that are shorter will be padded with zeros, and any sentences that are longer will be truncated. An easy way to choose a good value for the maximum sequence length is to look at the sentence length (in number of words) at different percentile positions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4jZidREzNPG",
        "outputId": "6424cd3e-e352-407a-df49-514b33e956c6"
      },
      "source": [
        "seq_lengths = np.array([len(s.split()) for s in sentences])\r\n",
        "print([(p, np.percentile(seq_lengths, p)) for p\r\n",
        "   in [75, 80, 90, 95, 99, 100]])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(75, 16.0), (80, 18.0), (90, 22.0), (95, 26.0), (99, 36.0), (100, 71.0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "459xFtX30E4G"
      },
      "source": [
        "The preceding blocks of code can be run interactively multiple times to choose good values of vocabulary size and maximum sequence length respectively. In our example, we have chosen to keep all the words (so vocab_size = 5271), and we have set our max_seqlen to 64."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96N4iiXg0UYI"
      },
      "source": [
        "Our next step is to create a dataset that our model can consume. We first use our trained tokenizer to convert each sentence from a sequence of words (sentences) to a sequence of integers (sentences_as_ints), where each corresponding integer is the index of the word in the tokenizer.word_index. It is then truncated and padded with zeros. The labels are also converted to a NumPy array labels_as_ints, and finally, we combine the tensors sentences_as_ints and labels_as_ints to form a TensorFlow dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G24autHW0FKO"
      },
      "source": [
        "max_seqlen = 64\r\n",
        "# create dataset\r\n",
        "sentences_as_ints = tokenizer.texts_to_sequences(sentences)\r\n",
        "sentences_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\r\n",
        "   sentences_as_ints, maxlen=max_seqlen)\r\n",
        "labels_as_ints = np.array(labels)\r\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\r\n",
        "   (sentences_as_ints, labels_as_ints))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYQpFIGA0paU"
      },
      "source": [
        "We want to set aside 1/3 of the dataset for evaluation. Of the remaining data, we will use 10% as an inline validation dataset that the model will use to gauge its own progress during training, and the remaining as the training dataset. Finally, we create batches of 64 sentences for each dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xnjbnfk700hU"
      },
      "source": [
        "dataset = dataset.shuffle(10000)\r\n",
        "test_size = len(sentences) // 3\r\n",
        "val_size = (len(sentences) - test_size) // 10\r\n",
        "test_dataset = dataset.take(test_size)\r\n",
        "val_dataset = dataset.skip(test_size).take(val_size)\r\n",
        "train_dataset = dataset.skip(test_size + val_size)\r\n",
        "batch_size = 64\r\n",
        "train_dataset = train_dataset.batch(batch_size)\r\n",
        "val_dataset = val_dataset.batch(batch_size)\r\n",
        "test_dataset = test_dataset.batch(batch_size)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOAzY22BDGAS"
      },
      "source": [
        "class SentimentAnalysisModel(tf.keras.Model):\r\n",
        "   def __init__(self, vocab_size, max_seqlen, **kwargs):\r\n",
        "       super(SentimentAnalysisModel, self).__init__(**kwargs)\r\n",
        "       self.embedding = tf.keras.layers.Embedding(\r\n",
        "           vocab_size, max_seqlen)\r\n",
        "       self.bilstm = tf.keras.layers.Bidirectional(\r\n",
        "           tf.keras.layers.LSTM(max_seqlen)\r\n",
        "       )\r\n",
        "       self.dense = tf.keras.layers.Dense(64, activation=\"relu\")\r\n",
        "       self.out = tf.keras.layers.Dense(1, activation=\"sigmoid\")\r\n",
        "   def call(self, x):\r\n",
        "       x = self.embedding(x)\r\n",
        "       x = self.bilstm(x)\r\n",
        "       x = self.dense(x)\r\n",
        "       x = self.out(x)\r\n",
        "       return x\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muOMlXy6FMzf"
      },
      "source": [
        "# set random seed\r\n",
        "tf.random.set_seed(42)\r\n",
        "# clean up log area\r\n",
        "data_dir = \"./data\"\r\n",
        "logs_dir = clean_logs(data_dir)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3g5Wa0uFM2T",
        "outputId": "1cb6ac72-fe08-4d46-a385-10b9520ddde6"
      },
      "source": [
        "# define model\r\n",
        "# vocab_size + 1 to account for PAD character\r\n",
        "model = SentimentAnalysisModel(vocab_size+1, max_seqlen)\r\n",
        "model.build(input_shape=(batch_size, max_seqlen))\r\n",
        "model.summary()\r\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sentiment_analysis_model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      multiple                  337408    \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection multiple                  66048     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              multiple                  8256      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              multiple                  65        \n",
            "=================================================================\n",
            "Total params: 411,777\n",
            "Trainable params: 411,777\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8W8So8bFM6q"
      },
      "source": [
        "# compile\r\n",
        "model.compile(\r\n",
        "    loss=\"binary_crossentropy\",\r\n",
        "    optimizer=\"adam\", \r\n",
        "    metrics=[\"accuracy\"]\r\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnSmiQ5DFjMN",
        "outputId": "0e19f038-6ffa-4163-8b8d-250354eb827f"
      },
      "source": [
        "# train\r\n",
        "best_model_file = os.path.join(data_dir, \"best_model.h5\")\r\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(best_model_file,\r\n",
        "    save_weights_only=True,\r\n",
        "    save_best_only=True)\r\n",
        "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\r\n",
        "num_epochs = 10\r\n",
        "history = model.fit(train_dataset, epochs=num_epochs, \r\n",
        "    validation_data=val_dataset,\r\n",
        "    callbacks=[checkpoint, tensorboard])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "29/29 [==============================] - 8s 142ms/step - loss: 0.6924 - accuracy: 0.5163 - val_loss: 0.6868 - val_accuracy: 0.6650\n",
            "Epoch 2/10\n",
            "29/29 [==============================] - 3s 95ms/step - loss: 0.6729 - accuracy: 0.6772 - val_loss: 0.5025 - val_accuracy: 0.8650\n",
            "Epoch 3/10\n",
            "29/29 [==============================] - 3s 94ms/step - loss: 0.4496 - accuracy: 0.8371 - val_loss: 0.1766 - val_accuracy: 0.9350\n",
            "Epoch 4/10\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.2238 - accuracy: 0.9280 - val_loss: 0.1308 - val_accuracy: 0.9600\n",
            "Epoch 5/10\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.1453 - accuracy: 0.9560 - val_loss: 0.1341 - val_accuracy: 0.9550\n",
            "Epoch 6/10\n",
            "29/29 [==============================] - 3s 96ms/step - loss: 0.1139 - accuracy: 0.9657 - val_loss: 0.0591 - val_accuracy: 0.9850\n",
            "Epoch 7/10\n",
            "29/29 [==============================] - 3s 96ms/step - loss: 0.0797 - accuracy: 0.9762 - val_loss: 0.0637 - val_accuracy: 0.9750\n",
            "Epoch 8/10\n",
            "29/29 [==============================] - 3s 97ms/step - loss: 0.0492 - accuracy: 0.9911 - val_loss: 0.0473 - val_accuracy: 0.9850\n",
            "Epoch 9/10\n",
            "29/29 [==============================] - 3s 96ms/step - loss: 0.0674 - accuracy: 0.9833 - val_loss: 0.0396 - val_accuracy: 0.9900\n",
            "Epoch 10/10\n",
            "29/29 [==============================] - 3s 98ms/step - loss: 0.0550 - accuracy: 0.9888 - val_loss: 0.0274 - val_accuracy: 0.9900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic3JxLCFDiu3"
      },
      "source": [
        "Our checkpoint callback has saved the best model based on the lowest value of validation loss, and we can now reload this for evaluation against our held out test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZjvoplkDhhf",
        "outputId": "abe32cfd-2a94-4533-b503-c4957eefa150"
      },
      "source": [
        "# evaluate with test set\r\n",
        "best_model = SentimentAnalysisModel(vocab_size+1, max_seqlen)\r\n",
        "best_model.build(input_shape=(batch_size, max_seqlen))\r\n",
        "best_model.load_weights(best_model_file)\r\n",
        "best_model.compile(\r\n",
        "    loss=\"binary_crossentropy\",\r\n",
        "    optimizer=\"adam\", \r\n",
        "    metrics=[\"accuracy\"]\r\n",
        ")\r\n",
        "\r\n",
        "test_loss, test_acc = best_model.evaluate(test_dataset)\r\n",
        "print(\"test loss: {:.3f}, test accuracy: {:.3f}\".format(test_loss, test_acc))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 2s 24ms/step - loss: 0.0326 - accuracy: 0.9885\n",
            "test loss: 0.034, test accuracy: 0.993\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtrAMttdFuo_",
        "outputId": "eda7371e-7399-4788-b9fe-9e644a636986"
      },
      "source": [
        "# predict on batches\r\n",
        "labels, predictions = [], []\r\n",
        "idx2word[0] = \"PAD\"\r\n",
        "is_first_batch = True\r\n",
        "for test_batch in test_dataset:\r\n",
        "    inputs_b, labels_b = test_batch\r\n",
        "    pred_batch = best_model.predict(inputs_b)\r\n",
        "    predictions.extend([(1 if p > 0.5 else 0) for p in pred_batch])\r\n",
        "    labels.extend([l for l in labels_b])\r\n",
        "    if is_first_batch:\r\n",
        "        for rid in range(inputs_b.shape[0]):\r\n",
        "            words = [idx2word[idx] for idx in inputs_b[rid].numpy()]\r\n",
        "            words = [w for w in words if w != \"PAD\"]\r\n",
        "            sentence = \" \".join(words)\r\n",
        "            print(\"{:d}\\t{:d}\\t{:s}\".format(labels[rid], predictions[rid], sentence))\r\n",
        "        is_first_batch = False\r\n",
        "\r\n",
        "print(\"accuracy score: {:.3f}\".format(accuracy_score(labels, predictions)))\r\n",
        "print(\"confusion matrix\")\r\n",
        "print(confusion_matrix(labels, predictions))\r\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\t0\tso don't go there if you are looking for good food\n",
            "1\t1\tportable and it works\n",
            "0\t0\tthe pan cakes everyone are raving about taste like a sugary disaster tailored to the palate of a six year old\n",
            "0\t0\twill not be back\n",
            "1\t1\tgreat it was new packaged nice works good no problems and it came in less time then i expected\n",
            "1\t1\tthis is the phone to get for 2005 i just bought my s710a and all i can say is wow\n",
            "0\t0\tthe lines the cuts the audio everything is wrong\n",
            "1\t1\tthis if the first movie i've given a 10 to in years\n",
            "0\t0\tall in all ha long bay was a bit of a flop\n",
            "0\t0\tspeaking of the music it is unbearably predictably and kitchy\n",
            "0\t0\tamazon sucks\n",
            "1\t1\tgreat charger\n",
            "0\t0\tthe commercials are the most misleading\n",
            "1\t1\tvery clear quality sound and you don't have to mess with the sound on your ipod since you have the sound buttons on the headset\n",
            "1\t1\ti ordered the voodoo pasta and it was the first time i'd had really excellent pasta since going gluten free several years ago\n",
            "1\t1\tit is wonderful and inspiring to watch and i hope that it gets released again on to video or dvd\n",
            "0\t0\t1 long lasting battery you don't have to recharge it as frequentyly as some of the flip phones 2\n",
            "1\t1\tperabo has a nice energy level and is obviously very comfortable in front of a camera\n",
            "0\t0\tthe worst one of the series\n",
            "0\t0\tthe fried rice was dry as well\n",
            "1\t1\tso we went to tigerlilly and had a fantastic afternoon\n",
            "1\t1\ti can't wait to go back\n",
            "1\t1\ti'm pleased\n",
            "1\t1\tit quit working after i'd used it for about 18 months so i just purchased another one because this is the best headset i've ever owned\n",
            "0\t0\ti'm returning them\n",
            "0\t0\tthis convention never worked well in the past and certainly doesn't work here\n",
            "0\t0\tthis is was due to the fact that it took 20 minutes to be acknowledged then another 35 minutes to get our food and they kept forgetting things\n",
            "1\t1\tthe chefs were friendly and did a good job\n",
            "1\t1\tadd betty white and jean smart and you have a great cast everyone played their parts really well\n",
            "0\t0\tso anyone near you will hear part of your conversation\n",
            "0\t0\tif you haven't choked in your own vomit by the end by all the cheap drama and worthless dialogue you've must have bored yourself to death with this waste of time\n",
            "1\t1\tonly pros large seating area nice bar area great simple drink menu the best brick oven pizza with homemade dough\n",
            "0\t0\tnow we were chosen to be tortured with this disgusting piece of blatant american propaganda\n",
            "0\t0\t1 10 and only because there is no setting for 0 10\n",
            "0\t0\ti'm really disappointed all i have now is a charger that doesn't work\n",
            "1\t1\tour server was very nice and attentive as were the other serving staff\n",
            "1\t1\tit has everything you could want suspense drama comedy confusing subplots native americans brain eating if you're looking for the be all end all of brainsucking movies look no further\n",
            "1\t1\ti saw this short film on hbo the other day and absolutely loved it\n",
            "0\t0\tmy phone sounded ok not great ok but my wife's phone was almost totally unintelligible she couldn't understand a word being said on it\n",
            "0\t0\tafter the first charge kept going dead after 12 minutes\n",
            "0\t0\ti only hear garbage for audio\n",
            "0\t0\tdo not waste your money here\n",
            "0\t0\ti would advise to not purchase this item it never worked very well\n",
            "0\t0\tthe plot well i said i'd let that one go\n",
            "1\t1\tthe pancake was also really good and pretty large at that\n",
            "1\t1\twe had a group of 70 when we claimed we would only have 40 and they handled us beautifully\n",
            "1\t1\tthe scenes are often funny and occasionally touching as the characters evaluate their lives and where they are going\n",
            "0\t0\tand if that isn't enough of a mess of a movie for you the picture is also marred with a constant use of studio sets and indoor exteriors\n",
            "1\t1\ti really hope the team behind this movie makes more movies and that they will continue to do so in their own some kinda weird style\n",
            "1\t1\tthis phone works great\n",
            "0\t0\tyou get what you pay for i guess\n",
            "1\t1\tit has all the features i want\n",
            "1\t1\tworks great\n",
            "1\t1\tto summarize the food was incredible nay transcendant but nothing brings me joy quite like the memory of the pneumatic condiment dispenser\n",
            "1\t1\tjamie foxx absolutely is ray charles\n",
            "1\t1\tthe chicken was deliciously seasoned and had the perfect fry on the outside and moist chicken on the inside\n",
            "0\t0\thowever i recently watched the whole thing again on dvd and i was completely struck by how extremely stupid the storyline was how it contained holes inconsistencies and frankly a whole lot of crap and how horrid the dancing was\n",
            "1\t1\tyou get incredibly fresh fish prepared with care\n",
            "0\t0\tsprint charges for this service\n",
            "0\t0\ti hate movies like that\n",
            "0\t0\tcumbersome design\n",
            "1\t1\tthis movie is well balanced with comedy and drama and i thoroughly enjoyed myself\n",
            "1\t1\tboth of the egg rolls were fantastic\n",
            "1\t1\tthe reception through this headset is excellent\n",
            "accuracy score: 0.997\n",
            "confusion matrix\n",
            "[[511   1]\n",
            " [  2 486]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}