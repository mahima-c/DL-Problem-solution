{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq without attention for machine translation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMcbSMP7X+IKhWcVb1GhMNf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahima-c/DL-Problem-solution/blob/main/seq2seq_without_attention_for_machine_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wr8XUflVWIBt"
      },
      "source": [
        "seq2seq without attention for machine translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_dFNHVHWUMq"
      },
      "source": [
        "To understand the seq2seq model in greater detail, we will look at an example of one that learns how to translate from English to French using the French-English bilingual dataset from the Tatoeba Project (1997-2019) [26]. The dataset contains approximately 167,000 sentence pairs. To make our training go faster, we will only consider the first 30,000 sentence pairs for our training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkcA_JWrWA-s"
      },
      "source": [
        "import nltk\r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "import shutil\r\n",
        "import tensorflow as tf\r\n",
        "import os\r\n",
        "import unicodedata\r\n",
        "import zipfile\r\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMDBA3DnWyH8"
      },
      "source": [
        "If you recall the structure of the seq2seq network, the input to the encoder is a sequence of English words. On the decoder side, the input is a set of French words, and the output is the sequence of French words offset by 1 timestep. The following function will download the zip file, expand it, and create the datasets described before.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCtyYXD0XHUg"
      },
      "source": [
        "The input is preprocessed to \"asciify\" the characters, separate out specific punctuations from their neighboring word, and remove all characters other than alphabets and these specific punctuation symbols. Finally, the sentences are converted to lowercase. Each English sentence is just converted to a single sequence of words. Each French sentence is converted into two sequences, one preceded by the BOS pseudo-word and the other followed by the end of sentence (EOS) pseudo-word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J77Z0HVNXHu_"
      },
      "source": [
        "def preprocess_sentence(sent):\r\n",
        "    sent = \"\".join([c for c in unicodedata.normalize(\"NFD\", sent) \r\n",
        "        if unicodedata.category(c) != \"Mn\"])\r\n",
        "    sent = re.sub(r\"([!.?])\", r\" \\1\", sent)\r\n",
        "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\r\n",
        "    sent = re.sub(r\"\\s+\", \" \", sent)\r\n",
        "    sent = sent.lower()\r\n",
        "    return sent\r\n",
        "\r\n",
        "\r\n",
        "def download_and_read(url, num_sent_pairs=30000):\r\n",
        "    local_file = url.split('/')[-1]\r\n",
        "    if not os.path.exists(local_file):\r\n",
        "        os.system(\"wget -O {:s} {:s}\".format(local_file, url))\r\n",
        "        with zipfile.ZipFile(local_file, \"r\") as zip_ref:\r\n",
        "            zip_ref.extractall(\".\")\r\n",
        "    local_file = os.path.join(\".\", \"fra.txt\")\r\n",
        "    en_sents, fr_sents_in, fr_sents_out = [], [], []\r\n",
        "    with open(local_file, \"r\") as fin:\r\n",
        "        for i, line in enumerate(fin):\r\n",
        "            en_sent, fr_sent,_ = line.strip().split('\\t')\r\n",
        "            en_sent = [w for w in preprocess_sentence(en_sent).split()]\r\n",
        "            fr_sent = preprocess_sentence(fr_sent)\r\n",
        "            fr_sent_in = [w for w in (\"BOS \" + fr_sent).split()]\r\n",
        "            fr_sent_out = [w for w in (fr_sent + \" EOS\").split()]\r\n",
        "            en_sents.append(en_sent)\r\n",
        "            fr_sents_in.append(fr_sent_in)\r\n",
        "            fr_sents_out.append(fr_sent_out)\r\n",
        "            if i >= num_sent_pairs - 1:\r\n",
        "                break\r\n",
        "    return en_sents, fr_sents_in, fr_sents_out"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L1ICzH4LiZ6"
      },
      "source": [
        "def clean_up_logs(data_dir):\r\n",
        "    checkpoint_dir = os.path.join(data_dir, \"checkpoints\")\r\n",
        "    if os.path.exists(checkpoint_dir):\r\n",
        "        shutil.rmtree(checkpoint_dir, ignore_errors=True)\r\n",
        "        os.makedirs(checkpoint_dir)\r\n",
        "    return checkpoint_dir"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L60h3AZWZVq6"
      },
      "source": [
        "NUM_SENT_PAIRS = 30000\r\n",
        "EMBEDDING_DIM = 256\r\n",
        "ENCODER_DIM, DECODER_DIM = 1024, 1024\r\n",
        "BATCH_SIZE = 64\r\n",
        "NUM_EPOCHS = 30\r\n",
        "\r\n",
        "tf.random.set_seed(42)\r\n",
        "\r\n",
        "data_dir = \"./data\"\r\n",
        "checkpoint_dir = clean_up_logs(data_dir)\r\n",
        "\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeGldrK26MGT"
      },
      "source": [
        "# data preparation\r\n",
        "download_url = \"http://www.manythings.org/anki/fra-eng.zip\"\r\n",
        "sents_en, sents_fr_in, sents_fr_out = download_and_read(download_url)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWq6Q-6qY7NP"
      },
      "source": [
        "Our next step is to tokenize our inputs and create the vocabulary. Since we have sequences in two different languages, we will create two different tokenizers and vocabularies, one for each language. The tf.keras framework provides a very powerful and versatile tokenizer class â€“ here we have set filters to an empty string and lower to False because we have already done what was needed for tokenization in our preprocess_sentence() function. The Tokenizer creates various data structures from which we can compute the vocabulary sizes and lookup tables that allow us to go from word to word index and back.\r\n",
        "\r\n",
        "Next we handle different length sequences of words by padding with zeros at the end, using the pad_sequences() function. Because our strings are fairly short, we do not do any truncation; we just pad to the maximum length of sentence that we have (8 words for English, and 16 words for French):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNL7T7gbY7oj",
        "outputId": "aa577831-6452-4bd5-e868-6837e1b1b4cd"
      },
      "source": [
        "tokenizer_en = tf.keras.preprocessing.text.Tokenizer(\r\n",
        "    filters=\"\", lower=False)\r\n",
        "tokenizer_en.fit_on_texts(sents_en)\r\n",
        "data_en = tokenizer_en.texts_to_sequences(sents_en)\r\n",
        "data_en = tf.keras.preprocessing.sequence.pad_sequences(\r\n",
        "    data_en, padding=\"post\")\r\n",
        "tokenizer_fr = tf.keras.preprocessing.text.Tokenizer(\r\n",
        "    filters=\"\", lower=False)\r\n",
        "tokenizer_fr.fit_on_texts(sents_fr_in)\r\n",
        "tokenizer_fr.fit_on_texts(sents_fr_out)\r\n",
        "data_fr_in = tokenizer_fr.texts_to_sequences(sents_fr_in)\r\n",
        "data_fr_in = tf.keras.preprocessing.sequence.pad_sequences(\r\n",
        "    data_fr_in, padding=\"post\")\r\n",
        "data_fr_out = tokenizer_fr.texts_to_sequences(sents_fr_out)\r\n",
        "data_fr_out = tf.keras.preprocessing.sequence.pad_sequences(\r\n",
        "    data_fr_out, padding=\"post\")\r\n",
        "vocab_size_en = len(tokenizer_en.word_index)\r\n",
        "vocab_size_fr = len(tokenizer_fr.word_index)\r\n",
        "word2idx_en = tokenizer_en.word_index\r\n",
        "idx2word_en = {v:k for k, v in word2idx_en.items()}\r\n",
        "word2idx_fr = tokenizer_fr.word_index\r\n",
        "idx2word_fr = {v:k for k, v in word2idx_fr.items()}\r\n",
        "print(\"vocab size (en): {:d}, vocab size (fr): {:d}\".format(\r\n",
        "    vocab_size_en, vocab_size_fr))\r\n",
        "maxlen_en = data_en.shape[1]\r\n",
        "maxlen_fr = data_fr_out.shape[1]\r\n",
        "print(\"seqlen (en): {:d}, (fr): {:d}\".format(maxlen_en, maxlen_fr))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size (en): 4359, vocab size (fr): 7590\n",
            "seqlen (en): 8, (fr): 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00o1kYPAKhQK"
      },
      "source": [
        "batch_size = 64\r\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\r\n",
        "    (data_en, data_fr_in, data_fr_out))\r\n",
        "dataset = dataset.shuffle(10000)\r\n",
        "test_size = NUM_SENT_PAIRS // 4\r\n",
        "test_dataset = dataset.take(test_size).batch(\r\n",
        "    batch_size, drop_remainder=True)\r\n",
        "train_dataset = dataset.skip(test_size).batch(\r\n",
        "    batch_size, drop_remainder=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhe0j7pvZ9-1"
      },
      "source": [
        "Our data is now ready to be used for training the seq2seq network, which we will define next. Our encoder is an Embedding layer followed by a GRU layer. The input to the encoder is a sequence of integers, which is converted to a sequence of embedding vectors of size embedding_dim. This sequence of vectors is sent to an RNN, which converts the input at each of the num_timesteps time steps to a vector of size encoder_dim. Only the output at the last time step is returned, as shown by the return_sequences=False."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfN9wFQmaWem"
      },
      "source": [
        "In our example network, we have chosen our embedding dimension to be 128, followed by the encoder and decoder RNN dimension of 1024 each. Note that we have to add 1 to the vocabulary size for both the English and French vocabularies to account for the PAD character that was added during the pad_sequences() step:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ku8yQJhyZ-VU"
      },
      "source": [
        "class Encoder(tf.keras.Model):\r\n",
        "    def __init__(self, vocab_size, embedding_dim, num_timesteps, \r\n",
        "            encoder_dim, **kwargs):\r\n",
        "        super(Encoder, self).__init__(**kwargs)\r\n",
        "        self.encoder_dim = encoder_dim\r\n",
        "        self.embedding = tf.keras.layers.Embedding(\r\n",
        "            vocab_size, embedding_dim, input_length=num_timesteps)\r\n",
        "        self.rnn = tf.keras.layers.GRU(\r\n",
        "            encoder_dim, return_sequences=False, return_state=True)\r\n",
        "\r\n",
        "    def call(self, x, state):\r\n",
        "        x = self.embedding(x)\r\n",
        "        x, state = self.rnn(x, initial_state=state)\r\n",
        "        return x, state\r\n",
        "\r\n",
        "    def init_state(self, batch_size):\r\n",
        "        return tf.zeros((batch_size, self.encoder_dim))\r\n",
        "\r\n",
        "\r\n",
        "class Decoder(tf.keras.Model):\r\n",
        "    def __init__(self, vocab_size, embedding_dim, num_timesteps,\r\n",
        "            decoder_dim, **kwargs):\r\n",
        "        super(Decoder, self).__init__(**kwargs)\r\n",
        "        self.decoder_dim = decoder_dim\r\n",
        "        self.embedding = tf.keras.layers.Embedding(\r\n",
        "            vocab_size, embedding_dim, input_length=num_timesteps)\r\n",
        "        self.rnn = tf.keras.layers.GRU(\r\n",
        "            decoder_dim, return_sequences=True, return_state=True)\r\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\r\n",
        "\r\n",
        "    def call(self, x, state):\r\n",
        "        x = self.embedding(x)\r\n",
        "        x, state = self.rnn(x, state)\r\n",
        "        x = self.dense(x)\r\n",
        "        return x, state"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXsrKSkW9c1l"
      },
      "source": [
        "embedding_dim = 256\r\n",
        "encoder_dim, decoder_dim = 1024, 1024\r\n",
        "encoder = Encoder(vocab_size_en+1, \r\n",
        "    embedding_dim, maxlen_en, encoder_dim)\r\n",
        "decoder = Decoder(vocab_size_fr+1, \r\n",
        "    embedding_dim, maxlen_fr, decoder_dim)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iZO-vBxa0Sa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f05c11ce-d42d-4761-eb0a-93c6f9f506e2"
      },
      "source": [
        "for encoder_in, decoder_in, decoder_out in train_dataset:\r\n",
        "   encoder_state = encoder.init_state(batch_size)\r\n",
        "   encoder_out, encoder_state = encoder(encoder_in, encoder_state)\r\n",
        "   decoder_state = encoder_state\r\n",
        "   decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\r\n",
        "   break\r\n",
        "print(\"encoder input          :\", encoder_in.shape)\r\n",
        "print(\"encoder output         :\", encoder_out.shape, \"state:\", encoder_state.shape)\r\n",
        "print(\"decoder output (logits):\", decoder_pred.shape, \"state:\", decoder_state.shape)\r\n",
        "print(\"decoder output (labels):\", decoder_out.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder input          : (64, 8)\n",
            "encoder output         : (64, 1024) state: (64, 1024)\n",
            "decoder output (logits): (64, 16, 7591) state: (64, 1024)\n",
            "decoder output (labels): (64, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4YnlNCX9mUE"
      },
      "source": [
        "This produces the following output, which is in line with our expectations. The encoder input is a batch of a sequence of integers, each sequence being of size 8, which is the maximum number of tokens in our English sentences, so its dimension is (batch_size, maxlen_en).\r\n",
        "\r\n",
        "The output of the encoder is a single tensor (return_sequences=False) of shape (batch_size, encoder_dim) and represents a batch of context vectors representing the input sentences. The encoder state tensor has the same dimensions. The decoder outputs are also a batch of sequence of integers, but the maximum size of a French sentence is 16; therefore, the dimensions are (batch_size, maxlen_fr). The decoder predictions are a batch of probability distributions across all time steps; hence the dimensions are (batch_size, maxlen_fr, vocab_size_fr+1), and the decoder state is the same dimension as the encoder state (batch_size, decoder_dim):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVV_clv0bcFe"
      },
      "source": [
        "Next we define the loss function. Because we padded our sentences, we don't want to bias our results by considering equality of pad words between the labels and predictions. Our loss function masks our predictions with the labels, so padded positions on the label are also removed from the predictions, and we only compute our loss using the non zero elements on both the label and predictions. This is done as follows:\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPqiu4lubcaP"
      },
      "source": [
        "def loss_fn(ytrue, ypred):\r\n",
        "    scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
        "    mask = tf.math.logical_not(tf.math.equal(ytrue, 0))\r\n",
        "    mask = tf.cast(mask, dtype=tf.int64)\r\n",
        "    loss = scce(ytrue, ypred, sample_weight=mask)\r\n",
        "    return loss\r\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfqbZBEDK0Ir"
      },
      "source": [
        "Because the seq2seq model is not easy to package into a simple Keras model, we have to handle the training loop manually as well. Our train_step() function handles the flow of data and computes the loss at each step, applies the gradient of the loss back to the trainable weights, and returns the loss.\r\n",
        "\r\n",
        "Notice that the training code is not quite the same as what was described in our discussion of the seq2seq model earlier. Here it appears that the entire decoder_input is fed in one go into the decoder to produce the output offset by one time step, whereas in the discussion, we said that this happens sequentially, where the token generated in the previous time step is used as the input to the next time step.\r\n",
        "\r\n",
        "This is a common technique used to train seq2seq networks, which is called Teacher Forcing, where the input to the decoder is the ground truth output instead of the prediction from the previous time step. This is preferred because it makes training faster, but also results in some degradation in prediction quality. To offset this, techniques such as Scheduled Sampling can be used, where the input is sampled randomly either from the ground truth or the prediction at the previous time step, based on some threshold (depends on the problem, but usually varies between 0.1 and 0.4):\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJecPJvrKHGD"
      },
      "source": [
        "@tf.function\r\n",
        "def train_step(encoder_in, decoder_in, decoder_out, encoder_state):\r\n",
        "   with tf.GradientTape() as tape:\r\n",
        "       encoder_out, encoder_state = encoder(encoder_in, encoder_state)\r\n",
        "       decoder_state = encoder_state\r\n",
        "       decoder_pred, decoder_state = decoder(\r\n",
        "           decoder_in, decoder_state)\r\n",
        "       loss = loss_fn(decoder_out, decoder_pred)\r\n",
        "  \r\n",
        "   variables = (encoder.trainable_variables + \r\n",
        "       decoder.trainable_variables)\r\n",
        "   gradients = tape.gradient(loss, variables)\r\n",
        "   optimizer.apply_gradients(zip(gradients, variables))\r\n",
        "   return loss"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtDbYj23K5ie"
      },
      "source": [
        "The predict() method is used to randomly sample a single English sentence from the dataset and use the model trained so far to predict the French sentence. For reference, the label French sentence is also displayed. The evaluate() method computes the BiLingual Evaluation Understudy (BLEU) score [35] between the label and prediction across all records in the test set. BLEU scores are generally used where multiple ground truth labels exist (we have only one), but compares up to 4-grams (n-grams with n=4) in both reference and candidate sentences. Both the predict() and evaluate() methods are called at the end of every epoch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9I2b6VMKLMB"
      },
      "source": [
        "def predict(encoder, decoder, batch_size, \r\n",
        "        sents_en, data_en, sents_fr_out, \r\n",
        "        word2idx_fr, idx2word_fr):\r\n",
        "    random_id = np.random.choice(len(sents_en))\r\n",
        "    print(\"input    : \",  \" \".join(sents_en[random_id]))\r\n",
        "    print(\"label    : \", \" \".join(sents_fr_out[random_id]))\r\n",
        "\r\n",
        "    encoder_in = tf.expand_dims(data_en[random_id], axis=0)\r\n",
        "    decoder_out = tf.expand_dims(sents_fr_out[random_id], axis=0)\r\n",
        "\r\n",
        "    encoder_state = encoder.init_state(1)\r\n",
        "    encoder_out, encoder_state = encoder(encoder_in, encoder_state)\r\n",
        "    decoder_state = encoder_state\r\n",
        "\r\n",
        "    decoder_in = tf.expand_dims(\r\n",
        "        tf.constant([word2idx_fr[\"BOS\"]]), axis=0)\r\n",
        "    pred_sent_fr = []\r\n",
        "    while True:\r\n",
        "        decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\r\n",
        "        decoder_pred = tf.argmax(decoder_pred, axis=-1)\r\n",
        "        pred_word = idx2word_fr[decoder_pred.numpy()[0][0]]\r\n",
        "        pred_sent_fr.append(pred_word)\r\n",
        "        if pred_word == \"EOS\":\r\n",
        "            break\r\n",
        "        decoder_in = decoder_pred\r\n",
        "    \r\n",
        "    print(\"predicted: \", \" \".join(pred_sent_fr))\r\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoHxSW75-rKQ"
      },
      "source": [
        "def evaluate_bleu_score(encoder, decoder, test_dataset, \r\n",
        "        word2idx_fr, idx2word_fr):\r\n",
        "\r\n",
        "    bleu_scores = []\r\n",
        "    smooth_fn = SmoothingFunction()\r\n",
        "    for encoder_in, decoder_in, decoder_out in test_dataset:\r\n",
        "        encoder_state = encoder.init_state(batch_size)\r\n",
        "        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\r\n",
        "        decoder_state = encoder_state\r\n",
        "        decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\r\n",
        "\r\n",
        "        # compute argmax\r\n",
        "        decoder_out = decoder_out.numpy()\r\n",
        "        decoder_pred = tf.argmax(decoder_pred, axis=-1).numpy()\r\n",
        "\r\n",
        "        for i in range(decoder_out.shape[0]):\r\n",
        "            ref_sent = [idx2word_fr[j] for j in decoder_out[i].tolist() if j > 0]\r\n",
        "            hyp_sent = [idx2word_fr[j] for j in decoder_pred[i].tolist() if j > 0]\r\n",
        "            # remove trailing EOS\r\n",
        "            ref_sent = ref_sent[0:-1]\r\n",
        "            hyp_sent = hyp_sent[0:-1]\r\n",
        "            bleu_score = sentence_bleu([ref_sent], hyp_sent, \r\n",
        "                smoothing_function=smooth_fn.method1)\r\n",
        "            bleu_scores.append(bleu_score)\r\n",
        "\r\n",
        "    return np.mean(np.array(bleu_scores))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjlK5PqoK-Id"
      },
      "source": [
        "The training loop is shown as follows. We will use the Adam optimizer for our model. We also set up a checkpoint so we can save our model after every 10 epochs. We then train the model for 250 epochs, and print out the loss, an example sentence and its translation, and the BLEU score computed over the entire test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cK97Y1OmK-he",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcf745d6-4c9b-472f-ab49-f333c489df9e"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\r\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\r\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\r\n",
        "                                encoder=encoder,\r\n",
        "                                decoder=decoder)\r\n",
        "num_epochs = 250\r\n",
        "eval_scores = []\r\n",
        "for e in range(num_epochs):\r\n",
        "   encoder_state = encoder.init_state(batch_size)\r\n",
        "   for batch, data in enumerate(train_dataset):\r\n",
        "       encoder_in, decoder_in, decoder_out = data\r\n",
        "       # print(encoder_in.shape, decoder_in.shape, decoder_out.shape)\r\n",
        "       loss = train_step(encoder_in, decoder_in, decoder_out, encoder_state)\r\n",
        "  \r\n",
        "   print(\"Epoch: {}, Loss: {:.4f}\".format(e + 1, loss.numpy()))\r\n",
        "   if e % 10 == 0:\r\n",
        "       checkpoint.save(file_prefix=checkpoint_prefix)\r\n",
        "  \r\n",
        "   predict(encoder, decoder, batch_size, sents_en, data_en,\r\n",
        "       sents_fr_out, word2idx_fr, idx2word_fr)\r\n",
        "   eval_score = evaluate_bleu_score(encoder, decoder, \r\n",
        "       test_dataset, word2idx_fr, idx2word_fr)\r\n",
        "   print(\"Eval Score (BLEU): {:.3e}\".format(eval_score))\r\n",
        "   # eval_scores.append(eval_score)\r\n",
        "checkpoint.save(file_prefix=checkpoint_prefix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 1.1656\n",
            "input    :  what a strange man !\n",
            "label    :  quel homme bizarre ! EOS\n",
            "predicted:  quel est ton pantalon ! EOS\n",
            "Eval Score (BLEU): 2.360e-02\n",
            "Epoch: 2, Loss: 0.8047\n",
            "input    :  i m a truck driver .\n",
            "label    :  je suis un chauffeur de camion . EOS\n",
            "predicted:  je suis un nouvel gars . EOS\n",
            "Eval Score (BLEU): 3.503e-02\n",
            "Epoch: 3, Loss: 0.6707\n",
            "input    :  she looks confused .\n",
            "label    :  elle a l air desorientee . EOS\n",
            "predicted:  elle a l air perplexe . EOS\n",
            "Eval Score (BLEU): 4.671e-02\n",
            "Epoch: 4, Loss: 0.4774\n",
            "input    :  get on a horse .\n",
            "label    :  enfourche un cheval ! EOS\n",
            "predicted:  prends un peu de ceux ci ! EOS\n",
            "Eval Score (BLEU): 6.051e-02\n",
            "Epoch: 5, Loss: 0.4084\n",
            "input    :  i was careless .\n",
            "label    :  j etais insouciante . EOS\n",
            "predicted:  j etais submerge . EOS\n",
            "Eval Score (BLEU): 7.898e-02\n",
            "Epoch: 6, Loss: 0.3073\n",
            "input    :  we need proof .\n",
            "label    :  il nous faut des preuves . EOS\n",
            "predicted:  nous avons besoin de plus . EOS\n",
            "Eval Score (BLEU): 9.364e-02\n",
            "Epoch: 7, Loss: 0.2783\n",
            "input    :  what s it all mean ?\n",
            "label    :  que signifie tout ceci ? EOS\n",
            "predicted:  comment va tout ceci ? EOS\n",
            "Eval Score (BLEU): 1.117e-01\n",
            "Epoch: 8, Loss: 0.2382\n",
            "input    :  i stood .\n",
            "label    :  je me suis tenue debout . EOS\n",
            "predicted:  je me suis tenu debout . EOS\n",
            "Eval Score (BLEU): 1.238e-01\n",
            "Epoch: 9, Loss: 0.1696\n",
            "input    :  we re all retired .\n",
            "label    :  nous sommes tous retraites . EOS\n",
            "predicted:  nous sommes toutes retraitees . EOS\n",
            "Eval Score (BLEU): 1.328e-01\n",
            "Epoch: 10, Loss: 0.1946\n",
            "input    :  that was amazing .\n",
            "label    :  c etait incroyable . EOS\n",
            "predicted:  c etait incroyable . EOS\n",
            "Eval Score (BLEU): 1.415e-01\n",
            "Epoch: 11, Loss: 0.1529\n",
            "input    :  was it necessary ?\n",
            "label    :  etait ce necessaire ? EOS\n",
            "predicted:  etait ce necessaire ? EOS\n",
            "Eval Score (BLEU): 1.465e-01\n",
            "Epoch: 12, Loss: 0.1252\n",
            "input    :  do you have a dog ?\n",
            "label    :  as tu un chien ? EOS\n",
            "predicted:  avez vous un chien ? EOS\n",
            "Eval Score (BLEU): 1.510e-01\n",
            "Epoch: 13, Loss: 0.1667\n",
            "input    :  it s not pertinent .\n",
            "label    :  ce n est pas pertinent . EOS\n",
            "predicted:  ce n est pas pertinent . EOS\n",
            "Eval Score (BLEU): 1.561e-01\n",
            "Epoch: 14, Loss: 0.1476\n",
            "input    :  tom wants to speak .\n",
            "label    :  tom voudrait parler . EOS\n",
            "predicted:  tom voudrait parler . EOS\n",
            "Eval Score (BLEU): 1.566e-01\n",
            "Epoch: 15, Loss: 0.1451\n",
            "input    :  let me take that .\n",
            "label    :  laisse moi prendre ca . EOS\n",
            "predicted:  laisse moi prendre ca . EOS\n",
            "Eval Score (BLEU): 1.590e-01\n",
            "Epoch: 16, Loss: 0.1295\n",
            "input    :  stand still !\n",
            "label    :  tiens toi tranquille ! EOS\n",
            "predicted:  pas de l ecart . EOS\n",
            "Eval Score (BLEU): 1.612e-01\n",
            "Epoch: 17, Loss: 0.1378\n",
            "input    :  he was very poor .\n",
            "label    :  il etait fort pauvre . EOS\n",
            "predicted:  il etait tres pauvre . EOS\n",
            "Eval Score (BLEU): 1.637e-01\n",
            "Epoch: 18, Loss: 0.1417\n",
            "input    :  i prepaid .\n",
            "label    :  j ai paye d avance . EOS\n",
            "predicted:  j ai paye d avance . EOS\n",
            "Eval Score (BLEU): 1.642e-01\n",
            "Epoch: 19, Loss: 0.1401\n",
            "input    :  i am human .\n",
            "label    :  je suis humain . EOS\n",
            "predicted:  je suis humain . EOS\n",
            "Eval Score (BLEU): 1.653e-01\n",
            "Epoch: 20, Loss: 0.1051\n",
            "input    :  tom followed me .\n",
            "label    :  tom m a suivi . EOS\n",
            "predicted:  tom m a suivi . EOS\n",
            "Eval Score (BLEU): 1.666e-01\n",
            "Epoch: 21, Loss: 0.1258\n",
            "input    :  that book is old .\n",
            "label    :  ce livre est vieux . EOS\n",
            "predicted:  ce livre est vieux . EOS\n",
            "Eval Score (BLEU): 1.672e-01\n",
            "Epoch: 22, Loss: 0.1340\n",
            "input    :  thanks for helping .\n",
            "label    :  merci de ton aide . EOS\n",
            "predicted:  merci pour l aide . EOS\n",
            "Eval Score (BLEU): 1.683e-01\n",
            "Epoch: 23, Loss: 0.1273\n",
            "input    :  what s this for ?\n",
            "label    :  a quoi ca sert ? EOS\n",
            "predicted:  qu est ce qu il y a a faire ? EOS\n",
            "Eval Score (BLEU): 1.677e-01\n",
            "Epoch: 24, Loss: 0.1324\n",
            "input    :  we all felt tired .\n",
            "label    :  nous nous sentions tous fatigues . EOS\n",
            "predicted:  nous nous sentions tous fatigues . EOS\n",
            "Eval Score (BLEU): 1.685e-01\n",
            "Epoch: 25, Loss: 0.1462\n",
            "input    :  you seem depressed .\n",
            "label    :  vous avez l air deprimees . EOS\n",
            "predicted:  vous avez l air deprime . EOS\n",
            "Eval Score (BLEU): 1.715e-01\n",
            "Epoch: 26, Loss: 0.1181\n",
            "input    :  i bought a red tie .\n",
            "label    :  j ai achete une cravate rouge . EOS\n",
            "predicted:  j ai achete une cravate rouge . EOS\n",
            "Eval Score (BLEU): 1.687e-01\n",
            "Epoch: 27, Loss: 0.1263\n",
            "input    :  burn it .\n",
            "label    :  brule la . EOS\n",
            "predicted:  brulez le . EOS\n",
            "Eval Score (BLEU): 1.704e-01\n",
            "Epoch: 28, Loss: 0.1037\n",
            "input    :  he has a maid .\n",
            "label    :  il a une servante . EOS\n",
            "predicted:  il a une femme de menage . EOS\n",
            "Eval Score (BLEU): 1.700e-01\n",
            "Epoch: 29, Loss: 0.1103\n",
            "input    :  we ve all seen it .\n",
            "label    :  nous l avons toutes vu . EOS\n",
            "predicted:  nous l avons toutes vu . EOS\n",
            "Eval Score (BLEU): 1.713e-01\n",
            "Epoch: 30, Loss: 0.1001\n",
            "input    :  take a look .\n",
            "label    :  regardez ! EOS\n",
            "predicted:  jetez un il ! EOS\n",
            "Eval Score (BLEU): 1.727e-01\n",
            "Epoch: 31, Loss: 0.0931\n",
            "input    :  i haven t decided .\n",
            "label    :  je n ai pas decide . EOS\n",
            "predicted:  je n ai pas decide . EOS\n",
            "Eval Score (BLEU): 1.726e-01\n",
            "Epoch: 32, Loss: 0.1196\n",
            "input    :  tom nodded .\n",
            "label    :  tom hocha la tete . EOS\n",
            "predicted:  tom hocha la tete . EOS\n",
            "Eval Score (BLEU): 1.727e-01\n",
            "Epoch: 33, Loss: 0.1273\n",
            "input    :  i taught french .\n",
            "label    :  j ai enseigne le francais . EOS\n",
            "predicted:  j ai enseigne le francais . EOS\n",
            "Eval Score (BLEU): 1.710e-01\n",
            "Epoch: 34, Loss: 0.1213\n",
            "input    :  do you get it ?\n",
            "label    :  vous pigez ? EOS\n",
            "predicted:  est ce que tu captes ? EOS\n",
            "Eval Score (BLEU): 1.717e-01\n",
            "Epoch: 35, Loss: 0.0881\n",
            "input    :  how do we stop tom ?\n",
            "label    :  comment arretons nous tom ? EOS\n",
            "predicted:  comment arretons nous tom ? EOS\n",
            "Eval Score (BLEU): 1.728e-01\n",
            "Epoch: 36, Loss: 0.1175\n",
            "input    :  you can have both .\n",
            "label    :  vous pouvez avoir les deux . EOS\n",
            "predicted:  vous pouvez avoir les deux . EOS\n",
            "Eval Score (BLEU): 1.723e-01\n",
            "Epoch: 37, Loss: 0.1144\n",
            "input    :  please be careful .\n",
            "label    :  veuillez etre prudentes . EOS\n",
            "predicted:  soyez prudent je vous prie . EOS\n",
            "Eval Score (BLEU): 1.719e-01\n",
            "Epoch: 38, Loss: 0.1136\n",
            "input    :  i m falling .\n",
            "label    :  je tombe . EOS\n",
            "predicted:  je tombe . EOS\n",
            "Eval Score (BLEU): 1.733e-01\n",
            "Epoch: 39, Loss: 0.1161\n",
            "input    :  we all cried a lot .\n",
            "label    :  nous avons tous beaucoup pleure . EOS\n",
            "predicted:  nous avons tous beaucoup pleure . EOS\n",
            "Eval Score (BLEU): 1.751e-01\n",
            "Epoch: 40, Loss: 0.1186\n",
            "input    :  fill in the blanks .\n",
            "label    :  remplissez les espaces libres . EOS\n",
            "predicted:  remplissez les espaces libres . EOS\n",
            "Eval Score (BLEU): 1.733e-01\n",
            "Epoch: 41, Loss: 0.1209\n",
            "input    :  it s your book .\n",
            "label    :  c est votre livre . EOS\n",
            "predicted:  c est ton livre . EOS\n",
            "Eval Score (BLEU): 1.733e-01\n",
            "Epoch: 42, Loss: 0.1181\n",
            "input    :  it s crowded today .\n",
            "label    :  il y a foule aujourd hui . EOS\n",
            "predicted:  il y a foule aujourd hui . EOS\n",
            "Eval Score (BLEU): 1.727e-01\n",
            "Epoch: 43, Loss: 0.0985\n",
            "input    :  i abhor violence .\n",
            "label    :  je deteste la violence . EOS\n",
            "predicted:  je deteste la violence . EOS\n",
            "Eval Score (BLEU): 1.736e-01\n",
            "Epoch: 44, Loss: 0.1461\n",
            "input    :  i owe you my life .\n",
            "label    :  je vous dois la vie . EOS\n",
            "predicted:  je te dois la vie . EOS\n",
            "Eval Score (BLEU): 1.749e-01\n",
            "Epoch: 45, Loss: 0.0813\n",
            "input    :  the book is white .\n",
            "label    :  le livre est blanc . EOS\n",
            "predicted:  le livre est blanc . EOS\n",
            "Eval Score (BLEU): 1.749e-01\n",
            "Epoch: 46, Loss: 0.1287\n",
            "input    :  i m a surgeon .\n",
            "label    :  je suis chirurgien . EOS\n",
            "predicted:  je suis chirurgien . EOS\n",
            "Eval Score (BLEU): 1.742e-01\n",
            "Epoch: 47, Loss: 0.0874\n",
            "input    :  i wrote a memo .\n",
            "label    :  j ai redige une note . EOS\n",
            "predicted:  j ai redige une note . EOS\n",
            "Eval Score (BLEU): 1.750e-01\n",
            "Epoch: 48, Loss: 0.1182\n",
            "input    :  i found these .\n",
            "label    :  j ai trouve ceux ci . EOS\n",
            "predicted:  j ai trouve celles ci . EOS\n",
            "Eval Score (BLEU): 1.744e-01\n",
            "Epoch: 49, Loss: 0.1105\n",
            "input    :  i said he could go .\n",
            "label    :  j ai dit qu il pourrait partir . EOS\n",
            "predicted:  j ai dit qu il pourrait partir . EOS\n",
            "Eval Score (BLEU): 1.736e-01\n",
            "Epoch: 50, Loss: 0.0947\n",
            "input    :  do you believe him ?\n",
            "label    :  est ce que tu le crois ? EOS\n",
            "predicted:  est ce que tu le crois ? EOS\n",
            "Eval Score (BLEU): 1.762e-01\n",
            "Epoch: 51, Loss: 0.1195\n",
            "input    :  it s in the garage .\n",
            "label    :  il est dans le garage . EOS\n",
            "predicted:  elle est dans le garage . EOS\n",
            "Eval Score (BLEU): 1.731e-01\n",
            "Epoch: 52, Loss: 0.0897\n",
            "input    :  don t you know how ?\n",
            "label    :  ne savez vous pas comment ? EOS\n",
            "predicted:  ne savez vous pas comment ? EOS\n",
            "Eval Score (BLEU): 1.746e-01\n",
            "Epoch: 53, Loss: 0.0928\n",
            "input    :  you need friends .\n",
            "label    :  vous avez besoin d amis . EOS\n",
            "predicted:  vous avez besoin d amis . EOS\n",
            "Eval Score (BLEU): 1.752e-01\n",
            "Epoch: 54, Loss: 0.1077\n",
            "input    :  i get it .\n",
            "label    :  j ai compris . EOS\n",
            "predicted:  j ai compris . EOS\n",
            "Eval Score (BLEU): 1.759e-01\n",
            "Epoch: 55, Loss: 0.0876\n",
            "input    :  i m old enough .\n",
            "label    :  je suis suffisamment vieux . EOS\n",
            "predicted:  je suis suffisamment grand . EOS\n",
            "Eval Score (BLEU): 1.745e-01\n",
            "Epoch: 56, Loss: 0.1183\n",
            "input    :  they all cheered .\n",
            "label    :  elles acclamerent toutes . EOS\n",
            "predicted:  elles acclamerent toutes . EOS\n",
            "Eval Score (BLEU): 1.755e-01\n",
            "Epoch: 57, Loss: 0.0786\n",
            "input    :  no one misses you .\n",
            "label    :  vous ne manquez a personne . EOS\n",
            "predicted:  tu ne manques a personne . EOS\n",
            "Eval Score (BLEU): 1.757e-01\n",
            "Epoch: 58, Loss: 0.0911\n",
            "input    :  don t be rude .\n",
            "label    :  ne sois pas impolie ! EOS\n",
            "predicted:  ne sois pas impolie ! EOS\n",
            "Eval Score (BLEU): 1.742e-01\n",
            "Epoch: 59, Loss: 0.0988\n",
            "input    :  i m not overweight .\n",
            "label    :  je ne suis pas en surpoids . EOS\n",
            "predicted:  je ne suis pas en surpoids . EOS\n",
            "Eval Score (BLEU): 1.753e-01\n",
            "Epoch: 60, Loss: 0.0977\n",
            "input    :  he is not in .\n",
            "label    :  il n est pas chez lui . EOS\n",
            "predicted:  il n est pas chez lui . EOS\n",
            "Eval Score (BLEU): 1.756e-01\n",
            "Epoch: 61, Loss: 0.0846\n",
            "input    :  he doesn t like us .\n",
            "label    :  il ne nous aime pas . EOS\n",
            "predicted:  il ne nous aime pas . EOS\n",
            "Eval Score (BLEU): 1.743e-01\n",
            "Epoch: 62, Loss: 0.1030\n",
            "input    :  do i have a fever ?\n",
            "label    :  est ce que j ai de la fievre ? EOS\n",
            "predicted:  est ce que j ai de la fievre ? EOS\n",
            "Eval Score (BLEU): 1.742e-01\n",
            "Epoch: 63, Loss: 0.0934\n",
            "input    :  you re killing me .\n",
            "label    :  tu me tues . EOS\n",
            "predicted:  vous me tuez . EOS\n",
            "Eval Score (BLEU): 1.750e-01\n",
            "Epoch: 64, Loss: 0.0919\n",
            "input    :  did you send them ?\n",
            "label    :  les as tu envoyes ? EOS\n",
            "predicted:  les as tu envoyes ? EOS\n",
            "Eval Score (BLEU): 1.740e-01\n",
            "Epoch: 65, Loss: 0.0613\n",
            "input    :  i felt depressed .\n",
            "label    :  je me suis senti deprime . EOS\n",
            "predicted:  je me suis senti deprime . EOS\n",
            "Eval Score (BLEU): 1.762e-01\n",
            "Epoch: 66, Loss: 0.0892\n",
            "input    :  did tom send you ?\n",
            "label    :  tom vous a envoye ? EOS\n",
            "predicted:  tom t a envoye ? EOS\n",
            "Eval Score (BLEU): 1.750e-01\n",
            "Epoch: 67, Loss: 0.0768\n",
            "input    :  you re no singer .\n",
            "label    :  tu n es pas chanteur . EOS\n",
            "predicted:  tu n es pas chanteuse . EOS\n",
            "Eval Score (BLEU): 1.753e-01\n",
            "Epoch: 68, Loss: 0.0817\n",
            "input    :  mary is my mother .\n",
            "label    :  mary est ma mere . EOS\n",
            "predicted:  mary est ma mere . EOS\n",
            "Eval Score (BLEU): 1.735e-01\n",
            "Epoch: 69, Loss: 0.0996\n",
            "input    :  it wasn t very fun .\n",
            "label    :  ce ne fut pas tres marrant . EOS\n",
            "predicted:  ca n a pas ete tres marrant . EOS\n",
            "Eval Score (BLEU): 1.766e-01\n",
            "Epoch: 70, Loss: 0.0973\n",
            "input    :  they said it s ok .\n",
            "label    :  elles ont dit que ca convenait . EOS\n",
            "predicted:  elles ont dit que c etait d accord . EOS\n",
            "Eval Score (BLEU): 1.763e-01\n",
            "Epoch: 71, Loss: 0.0842\n",
            "input    :  did you win ?\n",
            "label    :  l as tu emporte ? EOS\n",
            "predicted:  l avez vous emporte ? EOS\n",
            "Eval Score (BLEU): 1.749e-01\n",
            "Epoch: 72, Loss: 0.0919\n",
            "input    :  this is annoying .\n",
            "label    :  c est facheux . EOS\n",
            "predicted:  c est facheux . EOS\n",
            "Eval Score (BLEU): 1.756e-01\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}